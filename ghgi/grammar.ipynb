{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from parser import devulgarize, strip_html_tags\n",
    "from parser import units_group, TEXT_NUMBERS\n",
    "from parser import NAMES, QTYS, MODS, STRIPPED_WORDS\n",
    "\n",
    "class Parser:\n",
    "    # TODO: pull all of the functions below into the Parser class\n",
    "\n",
    "    UNIT = 'UNIT'\n",
    "    ALT_UNIT = 'ALT_UNIT'\n",
    "    EA_UNIT = 'EA_UNIT'\n",
    "    QUANTITY = 'QTY'\n",
    "    ALT_QUANTITY = 'ALT_QTY'\n",
    "    EA_QUANTITY = 'EA_QTY'\n",
    "    AMOUNT = 'AMT'\n",
    "    ALT_AMOUNT = 'ALT_AMT'\n",
    "    EA_AMOUNT = 'EA_AMT'\n",
    "    PLUS_AMOUNT = 'PLUS_AMT'\n",
    "    INGREDIENT = 'INGRED'\n",
    "    ALT_INGREDIENT = 'ALT_INGRED'\n",
    "\n",
    "    OPEN_PARENTHESES = ['(', '[']\n",
    "    CLOSE_PARENTHESES = [')', ']']\n",
    "\n",
    "\n",
    "units = r'({})([sei]*)$'.format(units_group)\n",
    "DISREGARD_HEADERS = [\n",
    "    r'accompaniments?:',\n",
    "    r'equipment:',\n",
    "    r'for .*:',\n",
    "    r'garnish(es)?:',\n",
    "    r'glass(ware)?:',\n",
    "    r'grill heat:',\n",
    "    r'ingredient info:',\n",
    "    r'note:',\n",
    "    r'serving suggestion(s)?:',\n",
    "    r'special equipment:',\n",
    "    r'test-kitchen tip:',\n",
    "    r'type of fire:',\n",
    "]\n",
    "\n",
    "def disregard(text: str) -> bool:\n",
    "    \"\"\" Return True if text isn't a \"real\" ingredient\n",
    "    Things that are marked optional remain TBD, but I lean towards disregarding these, too.\n",
    "    \"\"\"\n",
    "\n",
    "    if not text: # empty\n",
    "        return True\n",
    "    if text.endswith(':'):\n",
    "        # Ends in a colon \":\" almost always means it's a directive.\n",
    "        # There are a few cases where a quantity is provided as part of this,\n",
    "        # but this is very rare, and more often than not the quantity is redundant\n",
    "        # with subsequent entries. We'll live with the misses here.\n",
    "        return True\n",
    "    all_parens = r'^\\([^\\)]*\\)$'\n",
    "    if re.match(all_parens, text):\n",
    "        # entirely parenthetical, e.g. \"(Essential oil complement: orange)\"\n",
    "        return True\n",
    "\n",
    "    colon_anywhere = r'.*:.*'\n",
    "    if re.match(colon_anywhere, text):\n",
    "        # check for certain other directives, which typically include a colon, \n",
    "        # e.g. \"Equipment:...\", \"Accompaniment:...\", \"Ingredient info:...\"\n",
    "        lowered = text.lower()\n",
    "        # re.match only considers string start, which is what we want\n",
    "        if any([re.match(pttrn, lowered) for pttrn in DISREGARD_HEADERS]):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "gapless_units = r'(\\d)({})\\b'.format(units_group)\n",
    "def gap_units(text: str) -> str:\n",
    "    # insert a space between quantities and units, e.g. '6c' -> '6 c'\n",
    "    return re.sub(gapless_units, r'\\g<1> \\g<2>', text)\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess/standardize ingredient text in preparation for analysis:\n",
    "        * Strip leading/trailing whitespace\n",
    "        * Strip out any html tags\n",
    "            * At some point, we need to decide the \"correct\" way to handle inlined recipe links and whether we want to do anything about that\n",
    "        * Expand vulgar fractions, prepending a space, e.g. \"1Â¼\" -> \"1 1/4\" (nltk will take 1/4 as a number, huzzah!)\n",
    "        * Put a space between spaceless units, e.g. \"6c\" -> \"6 c\"\n",
    "        * Convert fractional numbers to decimals (?)\n",
    "    \"\"\"\n",
    "    text = text.strip() # remove leading/trailing whitespace\n",
    "    text = strip_html_tags(text)\n",
    "    text = devulgarize(text)\n",
    "    text = gap_units(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenize(text:str) -> list:\n",
    "    \"\"\" A wrapper on nltk.word_tokenize, except when\n",
    "    we see things like \"grams/3\" in the nltk tokens (e.g. from an ingredient that read\n",
    "    \"45 grams/3 ounces of oil\"), replace it with \"grams\", \"/\", \"3\" so we can use the\n",
    "    slash as an indicator of an alternative measure.\n",
    "    \"\"\"\n",
    "    # tokenize and tag the text usin nltk defaults\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        if '/' not in t:\n",
    "            out += [t]\n",
    "            continue\n",
    "\n",
    "        parts = t.split('/')\n",
    "        digits = len([el for el in parts if el[:1].isdigit() or el[-1:].isdigit()])\n",
    "        if digits == len(parts):\n",
    "            out += [t]\n",
    "            continue\n",
    "\n",
    "        for i, p in enumerate(parts):\n",
    "            if p: # don't include empty strings\n",
    "                out += [p]\n",
    "            if i < len(parts) - 1:\n",
    "                out += ['/']\n",
    "\n",
    "    out = [numerify(el) for el in out]\n",
    "    out = [separate_qualifier_units(el) for el in out]\n",
    "    out = [e for el in out for e in el]\n",
    "\n",
    "    return out\n",
    "\n",
    "def rangeify(tokens: list) -> list:\n",
    "    \"\"\" Convert ranged numbers to their midpoints, e.g.\n",
    "    `6 to 8` -> `7`. This does *not* range units, e.g. it excludes\n",
    "    things like `6 cups to 2 gallons` which is done elsewhere tbd. \n",
    "    Trailing dashes are preserved.\n",
    "    Covered formats:\n",
    "        `6 to 8`\n",
    "        `6-to-8`\n",
    "        `6-to-8-`\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_value = None\n",
    "    range_active = False\n",
    "    for token in tokens:\n",
    "        # look for numbers, numbers ending in dashes\n",
    "        dash = ''\n",
    "        if token[-1] == '-':\n",
    "            dash = '-'\n",
    "            token = token[:-1]\n",
    "\n",
    "        if any([cc in token for cc in ['-to-', '-or-']]):\n",
    "            cc = '-to-' if '-to-' in token else '-or-'\n",
    "            parts = token.split(cc)\n",
    "            try:\n",
    "                midpoint = (float(parts[0]) + float(parts[1])) / 2\n",
    "                out += [str(midpoint) + dash]\n",
    "            except ValueError:\n",
    "                out += [token + dash]\n",
    "            range_active = False\n",
    "            prev_value = None\n",
    "\n",
    "        elif token in ['or', 'to']:\n",
    "            range_active = bool(prev_value)\n",
    "            out += [token + dash]\n",
    "\n",
    "        else: # see if it's a number\n",
    "            try:\n",
    "                val = float(token)\n",
    "                if range_active:\n",
    "                    midpoint = (val + prev_value) / 2\n",
    "                    out[-2:] = [str(midpoint) + dash]\n",
    "                    prev_value = None\n",
    "                else:\n",
    "                    out += [str(val) + dash]\n",
    "                    prev_value = val\n",
    "            except ValueError:\n",
    "                out += [token + dash]\n",
    "                prev_value = None\n",
    "            range_active = False\n",
    "\n",
    "    return out\n",
    "\n",
    "def decimate(tokens: list) -> list:\n",
    "    \"\"\" Convert fractions to decimals.\n",
    "    Yes, the method name is cheeky.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    prev_digit = None\n",
    "    for i, t in enumerate(tokens):\n",
    "        dash = ''\n",
    "        if t.endswith('-'):\n",
    "            t = t[:-1]\n",
    "            dash = '-'\n",
    "\n",
    "        parts = t.split('/')\n",
    "        if len(parts) != 2:\n",
    "            out += [t+dash]\n",
    "        \n",
    "        elif all([p.isdigit() for p in parts]):\n",
    "            # `1/2`\n",
    "            val = float(parts[0])/float(parts[1])\n",
    "            if prev_digit:\n",
    "                # sum and overwrite\n",
    "                val += prev_digit\n",
    "                out[-1] = str(val) + dash\n",
    "            else:\n",
    "                out += [str(val) + dash]\n",
    "            \n",
    "        elif parts[1].isdigit() and len(parts[0].split('-')) == 2 and all([p.isdigit() for p in parts[0].split('-')]):\n",
    "            # `1-1/2`\n",
    "            denominator = float(parts[1])\n",
    "            whole, numerator = parts[0].split('-')\n",
    "            val = float(whole) + float(numerator)/denominator\n",
    "            out += [str(val) + dash]\n",
    "\n",
    "        else:\n",
    "            out += [t+dash]\n",
    "\n",
    "        prev_digit = float(t) if t.isdigit() else None\n",
    "\n",
    "    return out\n",
    "\n",
    "def numerify(text: str) -> list:\n",
    "    \"\"\" Convert text numbers to digits \"\"\"\n",
    "    tokens = text.split('-')\n",
    "\n",
    "    if len(tokens) == 1:\n",
    "        # no dashes present; tokens[0] == text\n",
    "        return str(TEXT_NUMBERS.get(text.lower(), text))\n",
    "    elif not any([token.lower() in TEXT_NUMBERS for token in tokens]):\n",
    "        # no numbers\n",
    "        return text\n",
    "\n",
    "    # We have numeric text, which should only be converted to digits if\n",
    "    # it's being used as a quantity (vs, most notably, five-spice powder)\n",
    "    # or a range of quantities.\n",
    "    # This means the token after a number must be either a unit, a range\n",
    "    # indicator (to, or), or empty before we will convert it\n",
    "\n",
    "    if len(tokens) == 2:\n",
    "        if (not tokens[1]) or re.match(units, tokens[1]):\n",
    "            # e.g. \"five-\", \"six-ounce\"\n",
    "            tokens[0] = TEXT_NUMBERS.get(tokens[0].lower(), tokens[0])\n",
    "\n",
    "    elif len(tokens) == 4 and re.match(units, tokens[-1]) and tokens[1].lower() in ['or', 'to']:\n",
    "        # e.g. \"five-to-six-ounce\"\n",
    "        tokens = [TEXT_NUMBERS.get(token.lower(), token) for token in tokens]\n",
    "\n",
    "    return '-'.join([str(t) for t in tokens])\n",
    "\n",
    "def separate_qualifier_units(text: str) -> list:\n",
    "    tokens = text.split('-')\n",
    "    if len(tokens) == 1:\n",
    "        return [text]\n",
    "\n",
    "    if re.match(units, tokens[-1]):\n",
    "        unit = tokens[-1]\n",
    "        tokens[-1] = ''\n",
    "        return '-'.join(tokens), unit\n",
    "    \n",
    "    return [text]\n",
    "\n",
    "def tag_units_and_quantities(tokens: list) -> list:\n",
    "    # tag digit (CD) tokens with the QTY type\n",
    "    # tag units tokens with the UNITS type\n",
    "    out = []\n",
    "    for token in tokens:\n",
    "        if token[1] == 'CD': # digit\n",
    "            # make sure it's actually a digit\n",
    "            try:\n",
    "                float(token[0])\n",
    "                out += [(token[0], Parser.QUANTITY)]\n",
    "            except ValueError:\n",
    "                numeric = ''.join([s for s in token[0] if s in '1234567890.'])\n",
    "                try:\n",
    "                    float(numeric)\n",
    "                    out += [(numeric, Parser.QUANTITY)]\n",
    "                except ValueError:\n",
    "                    out += [token]\n",
    "        elif re.match(units, token[0]):\n",
    "            out += [(token[0], Parser.UNIT)]\n",
    "        else:\n",
    "            out += [token]\n",
    "    return out\n",
    "\n",
    "def coalesce_units(tokens: list) -> list:\n",
    "    # replace [UNIT]/[UNIT]/*, [UNIT] or [UNIT], and [UNIT], [UNIT] with first unit\n",
    "    out = []\n",
    "    prev_unit = False\n",
    "    for token in tokens:\n",
    "        if token[1] == Parser.UNIT:\n",
    "            if prev_unit:\n",
    "                trunc = False\n",
    "                while out[-1][0] in ['/', 'or', ',']:\n",
    "                    out = out[:-1]\n",
    "                    trunc = True\n",
    "                if trunc:\n",
    "                    continue\n",
    "            else:\n",
    "                prev_unit = True\n",
    "        elif prev_unit and token[0] not in ['/', 'or', ',']:\n",
    "            prev_unit = False\n",
    "        out += [token]\n",
    "    return out\n",
    "\n",
    "\n",
    "def tag_amounts(tokens: list) -> list:\n",
    "    # tag quantities that are qualifiers, as identified by a trailing dash,\n",
    "    # tag their subsequent units and EA_UNITS, and aggregate them with\n",
    "    # their units into a EA_AMOUNT. tag non-qualifier amounts as AMOUNT.\n",
    "    out = []\n",
    "    qqr = r'[\\d\\.]+-'\n",
    "    approximators = ['about', 'approx', 'approx.', 'approximately']\n",
    "    skip = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        if re.match(qqr, token[0]):\n",
    "            # drop the trailing dash and mark as an EA_QUANTITY\n",
    "            out += [(token[0][:-1], Parser.EA_QUANTITY)]\n",
    "        elif token[1] == Parser.UNIT and i > 0 and out[-1][1] == Parser.QUANTITY:\n",
    "            if len(tokens) > i + 1 and tokens[i+1][0][:4] in ['each']:\n",
    "                out[-1] = [[(out[-1][0], Parser.EA_QUANTITY), (token[0], Parser.EA_UNIT)], Parser.EA_AMOUNT]\n",
    "                # also check for a preceding paired amount and convert to an EA_AMOUNT\n",
    "                # this may not cover all possible cases!\n",
    "                if len(out) > 2 and out[-2][0] in ['/', 'or'] and out[-3][1] == Parser.AMOUNT:\n",
    "                    # mark its paired amount as an EA_AMOUNT\n",
    "                    out[-3] = [out[-3][0], Parser.EA_AMOUNT]\n",
    "                # look for any preceding approximator and remove it\n",
    "                if len(out) > 1 and out[-2][0] in approximators:\n",
    "                    out = out[:-3] + [out[-1]]\n",
    "                skip = True # skip the following 'each'\n",
    "            elif len(out) > 2 and out[-2][0] in approximators:\n",
    "                # check for 'about' or 'approx*' before this, which indicates an ALT_AMOUNT\n",
    "                out[-2] = [[(out[-1][0], Parser.ALT_QUANTITY), (token[0], Parser.ALT_UNIT)], Parser.ALT_AMOUNT]\n",
    "                out = out[:-1]\n",
    "            else:\n",
    "                out[-1] = [[out[-1], token], Parser.AMOUNT]\n",
    "        elif token[1] == Parser.UNIT and i > 0 and out[-1][1] == Parser.EA_QUANTITY:\n",
    "                out[-1] = [[out[-1], (token[0], Parser.EA_UNIT)], Parser.EA_AMOUNT]\n",
    "        else:\n",
    "            out += [token]\n",
    "    return out\n",
    "\n",
    "def strip_qualifier_parens(tokens: list) -> list:\n",
    "    # Remove parentheticals, preserving only qualifier amount(s).\n",
    "    # if the parenthetical starts with \"or AMT\", remove the parentheses but\n",
    "    # otherwise preserve it\n",
    "    paren_indexes = []\n",
    "    cur_paren_index = []\n",
    "    parens_only = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[0] in Parser.OPEN_PARENTHESES:\n",
    "            cur_paren_index += [i]\n",
    "            if len(tokens) > i + 2:\n",
    "                if type(tokens[i+1][0]) is str and tokens[i+1][0].lower() in ['or']:\n",
    "                    if tokens[i+2][1] in [Parser.AMOUNT, Parser.ALT_UNIT, Parser.EA_AMOUNT]:\n",
    "                        parens_only = True\n",
    "        elif token[0] in Parser.CLOSE_PARENTHESES:\n",
    "            if cur_paren_index:\n",
    "                cur_paren_index += [i]\n",
    "                paren_indexes += cur_paren_index\n",
    "                cur_paren_index = [] # reset\n",
    "                parens_only = False\n",
    "        elif cur_paren_index and token[1] not in [Parser.EA_AMOUNT, Parser.ALT_AMOUNT, Parser.AMOUNT] and not parens_only:\n",
    "            cur_paren_index += [i]\n",
    "\n",
    "    return [t for i, t in enumerate(tokens) if i not in paren_indexes]\n",
    "\n",
    "def complete_amounts(tokens:list) -> list:\n",
    "    # pull QTY or UNIT values back to orphaned partners, and fill in\n",
    "    # the blanks if none are found\n",
    "    # do not complete orphaned QTYs in parentheticals, as this is usually\n",
    "    # not actually quantity information.\n",
    "    out = []\n",
    "    qty_index = None\n",
    "    unit_index = None\n",
    "    any_qty = False\n",
    "    any_unit = False\n",
    "    any_amt = False\n",
    "    parenthetical = False\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[0] in Parser.OPEN_PARENTHESES:\n",
    "            parenthetical = True\n",
    "        elif parenthetical and token[0] in Parser.CLOSE_PARENTHESES:\n",
    "            parenthetical = False\n",
    "        if token[1] == Parser.UNIT:\n",
    "            any_unit = True\n",
    "            if qty_index is not None:\n",
    "                # this is the unit for the existing qty\n",
    "                out[qty_index] = [[out[qty_index], token], Parser.AMOUNT]\n",
    "                qty_index = None\n",
    "                continue\n",
    "            elif unit_index is not None:\n",
    "                # default to qty 1 for existing unit_index\n",
    "                out[unit_index] = [[('1', Parser.QUANTITY), out[unit_index]], Parser.AMOUNT]\n",
    "            unit_index = len(out)\n",
    "            # out += [token]\n",
    "        elif token[1] == Parser.QUANTITY and not parenthetical:\n",
    "            any_qty = True\n",
    "            if qty_index is not None:\n",
    "                # existing qty is an orphan; default unit to `ea`\n",
    "                out[qty_index] = [[out[qty_index], ('ea', Parser.UNIT)], Parser.AMOUNT]\n",
    "            qty_index = len(out)\n",
    "            # out += [token]\n",
    "        elif token[1] == Parser.AMOUNT:\n",
    "            any_amt = True\n",
    "        out += [token]\n",
    "\n",
    "\n",
    "    # clean up any units/qtys left hanging at the end\n",
    "    if qty_index is not None:\n",
    "        out[qty_index] = [[out[qty_index], ('ea', Parser.UNIT)], Parser.AMOUNT]\n",
    "\n",
    "    if unit_index is not None:\n",
    "        out[unit_index] = [[('1', Parser.QUANTITY), out[unit_index]], Parser.AMOUNT]\n",
    "        if unit_index > 0 and out[unit_index-1][1] == Parser.EA_AMOUNT:\n",
    "            # if hanging unit was preceded by an EA_AMOUNT, flip the AMOUNT before \n",
    "            # the EA_AMOUNT\n",
    "            out[unit_index-1:unit_index+1] = [out[unit_index], out[unit_index-1]]\n",
    "\n",
    "    if not (any_amt or any_unit or any_qty):\n",
    "        out = [[[('1', Parser.QUANTITY), ('ea', Parser.UNIT)], Parser.AMOUNT]] + out\n",
    "\n",
    "    return out\n",
    "\n",
    "def merge_amounts(tokens: list, merge_flavor) -> list:\n",
    "    # link ALT_AMOUNTs to their parent AMOUNT or EA_AMOUNT,\n",
    "    # and link EA_AMOUNTS to their parent AMOUNT\n",
    "    out = []\n",
    "    prev_par_index = None\n",
    "\n",
    "    for _, token in enumerate(tokens):\n",
    "        if not token[1] == merge_flavor:\n",
    "            out += [token]\n",
    "\n",
    "        if token[1] == merge_flavor:\n",
    "            if prev_par_index is not None:\n",
    "                out[prev_par_index][0] += [token]\n",
    "            else:\n",
    "                out += [token]\n",
    "        elif token[1] in [Parser.AMOUNT, Parser.EA_AMOUNT]:\n",
    "            # order of this and previous elif matter because EA_AMOUNT\n",
    "            # cannot be a parent if its the merge_flavor\n",
    "            prev_par_index = len(out) - 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def plus_amounts(tokens: list) -> list:\n",
    "    # If we find the pattern [AMT, 'plus', AMT], append the second\n",
    "    # AMT to the first one and change its type to Parser.PLUS_AMOUNT\n",
    "    out = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[1] in [Parser.AMOUNT]:\n",
    "            # handle this via lookbehind\n",
    "            if i > 1:\n",
    "                prev_token = tokens[i-1][0]\n",
    "                if type(prev_token) is str and prev_token.lower() in ['plus']:\n",
    "                    prev_prev_token = tokens[i-2]\n",
    "                    if prev_prev_token[1] in [Parser.AMOUNT]:\n",
    "                        out[-2][0] += [(token[0], Parser.PLUS_AMOUNT)]\n",
    "                        out = out[:-1]\n",
    "                        continue\n",
    "        out += [token]\n",
    "    return out\n",
    "\n",
    "def strip_remnants(tokens: list) -> list:\n",
    "    # Remove leftover things like slashes that separated amounts.\n",
    "    # Currently looks for slashes and 'of'\n",
    "    out = []\n",
    "    prev_flavor = None\n",
    "\n",
    "    for token in tokens:\n",
    "        if token[0] in ['/', 'of'] and prev_flavor in [Parser.AMOUNT, Parser.EA_AMOUNT, Parser.ALT_AMOUNT]:\n",
    "            continue\n",
    "        out += [token]\n",
    "        prev_flavor = token[1]\n",
    "\n",
    "    return out\n",
    "\n",
    "def label_ingredients(tokens: list) -> list:\n",
    "    # Sometimes an ingredient line contains two separate ingredients,\n",
    "    # usually because the second one is provided as an alternative.\n",
    "    # Here, we wrap ingredients in either Parser.INGREDIENT or Parser.ALT_INGREDIENT\n",
    "    # tags if full ingredients are separated by an 'or' token.\n",
    "    # We also want to pull amounts to the front of ingredients if they don't already\n",
    "    # have one.\n",
    "    # This has quite limited capabilities and will need to be evolved.\n",
    "    ingred = []\n",
    "    ingred_amt = False\n",
    "    ingred_name = False\n",
    "    ingred_has_lead_amt = False\n",
    "    or_index = None\n",
    "    out = []\n",
    "    # print('LABEL:', tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token[1] in [Parser.AMOUNT, Parser.ALT_AMOUNT, Parser.EA_AMOUNT]:\n",
    "            if ingred_amt and ingred_name and (or_index is not None):\n",
    "                # we've already parsed a full ingredient; start a new one\n",
    "                if out:\n",
    "                    # print('appending as ALT_INGREDIENT:', ingred)\n",
    "                    out += [[ingred, Parser.ALT_INGREDIENT]]\n",
    "                else:\n",
    "                    del ingred[or_index]\n",
    "                    # print('appending as INGREDIENT:', ingred)\n",
    "                    out += [[ingred, Parser.INGREDIENT]]\n",
    "                ingred = [] # reset\n",
    "                ingred_name = False\n",
    "                or_index = None\n",
    "                ingred_has_lead_amt = False\n",
    "            ingred_amt = True\n",
    "        elif type(token[1]) is str:\n",
    "            if token[1].startswith('NN'):\n",
    "                # print('ingred_name is TRUE')\n",
    "                ingred_name = True\n",
    "            elif token[0].lower() == 'or' and ingred_name and ingred_amt:\n",
    "                # only count this if we've seen the rest of the ingredient\n",
    "                or_index = len(ingred)\n",
    "\n",
    "        if token[1] in [Parser.AMOUNT, Parser.ALT_AMOUNT, Parser.EA_AMOUNT] and not ingred_has_lead_amt:\n",
    "            # move amount to the front only if the ingred doesn't already have an amount\n",
    "            # print('prepending token to ingred:', token)\n",
    "            ingred = [token] + ingred # prepend egads what?\n",
    "            ingred_has_lead_amt = True\n",
    "        else:\n",
    "            # print('appending token to ingred:', token)\n",
    "            ingred += [token]\n",
    "    \n",
    "    if ingred:\n",
    "        # print('appending remainder ingred')\n",
    "        flavor = Parser.ALT_INGREDIENT if out else Parser.INGREDIENT\n",
    "        # print('appending ingred', ingred, 'as flavor', flavor)\n",
    "        out += [[ingred, flavor]]\n",
    "    # print('out value is', out)    \n",
    "    return out\n",
    "\n",
    "def focus(ingredient: list) -> list:\n",
    "    # Remove extraneous things like prep mods and unhelpful descriptions.\n",
    "    # This is mostly done using parts of speech and their position in the ingredient.\n",
    "    # We call this recursively as we eliminate unhelpful bits.\n",
    "    ingred = ingredient[0]\n",
    "    pos_distractions = ['VBD', 'VBN', 'IN', 'DT']\n",
    "    distractions = ['preferably']\n",
    "\n",
    "    if ingred[-1][0] in [',', ';']:\n",
    "        # eliminate dangling punctuation\n",
    "        ingred = ingred[:-1]\n",
    "        return focus([ingred, ingredient[1]])\n",
    "\n",
    "    first_noun = [i for i in range(len(ingred)) if type(ingred[i][1]) is str and ingred[i][1].startswith('NN')]\n",
    "    first_noun = min(first_noun) if first_noun else 0\n",
    "\n",
    "    # find the comma indexes, which often delineate phrases\n",
    "    commas = [i for i in range(len(ingred)) if ingred[i][0] in [','] and i > first_noun]\n",
    "    if commas:\n",
    "        # TODO: this bails as soon as it finds one useful phrase; it should continue to\n",
    "        # backtrack and see if there's anything else to snip out!\n",
    "        phrase = ingred[commas[-1]+1:]\n",
    "        if phrase[0][1] in pos_distractions or phrase[-1][1] in pos_distractions or phrase[0][0] in distractions:\n",
    "            ingred = ingred[:commas[-1]]\n",
    "            return focus([ingred, ingredient[1]])\n",
    "\n",
    "\n",
    "    # fix cruft in the lede phrase\n",
    "    noun_seen = False\n",
    "    distraction_start = None\n",
    "    lede_pos_distractions = ['VBD', 'VBN', 'DT']\n",
    "    lede_destractions = ['in']\n",
    "    \n",
    "    for i, el in enumerate(ingred):\n",
    "        if not noun_seen and type(el[1]) is str and el[1].startswith('NN'):\n",
    "            noun_seen = True\n",
    "        elif noun_seen and (distraction_start is None) and (el[1] in lede_pos_distractions or el[0] in lede_destractions):\n",
    "            # print('distraction found:', el[1])\n",
    "            distraction_start = i\n",
    "        elif noun_seen and el[0] == ',':\n",
    "            break\n",
    "\n",
    "    if distraction_start is not None:\n",
    "        # print('ingred[:distraction_start]', ingred[:distraction_start])\n",
    "        # print('ingred[i:]', ingred[i:])\n",
    "        ingred = ingred[:distraction_start] + ingred[i+1:]\n",
    "\n",
    "    # there can also be cruft prior to the lede phrase; remove it\n",
    "    prior_phrases = [i for i in range(len(ingred)) if ingred[i][0] in [','] and i < first_noun]\n",
    "    non_amts = [i for i in range(len(ingred)) if ingred[i][1] not in [Parser.AMOUNT, Parser.EA_AMOUNT, Parser.ALT_AMOUNT]]\n",
    "    non_amt_start = min(non_amts) if non_amts else None\n",
    "    if prior_phrases and (non_amt_start is not None):\n",
    "        # print('pre-cruft available to kill!!!')\n",
    "        phrase = ingred[non_amt_start:prior_phrases[0]+1]\n",
    "        # print('phrase', phrase)\n",
    "        if len(phrase)==1 or phrase[0][1] in distractions or phrase[-1][1] in distractions:\n",
    "            # print('snipping!')\n",
    "            # snip it out\n",
    "            ingred = ingred[:non_amt_start] + ingred[prior_phrases[0]+1:]\n",
    "            # print(ingred)\n",
    "            return focus([ingred, ingredient[1]])\n",
    "\n",
    "\n",
    "    return [ingred, ingredient[1]]\n",
    "\n",
    "def unstop(ingredient: list) -> list:\n",
    "    # remove stopwords and prep mods from parsed ingredient\n",
    "    from parser import STOPWORDS, PREP_MODS\n",
    "    out = []\n",
    "    prev_token = None\n",
    "    for token in ingredient[0]:\n",
    "        if type(token[0]) is str and (token[0] in STOPWORDS or token[0] in PREP_MODS):\n",
    "            continue\n",
    "        if token[0] == ',' and prev_token in [',', None]:\n",
    "            # print('setting prev_token to (inside)', token[0])\n",
    "            prev_token = token[0]\n",
    "            continue\n",
    "        out += [token]\n",
    "        if type(token[0]) is str:\n",
    "            # print('setting prev_token to', token[0])\n",
    "            prev_token = token[0]\n",
    "    # clean up dangling cruft left by removal of stopwords\n",
    "    while out and out[-1][1] in ['CC', ',']:\n",
    "        del out[-1]\n",
    "    return [out, ingredient[1]]\n",
    "\n",
    "def retag(ingredient:list) -> list:\n",
    "    # we want to parse as big of chunks as we can to maximize the\n",
    "    # info we give to nltk. \n",
    "    # Find any ingredient bits that aren't strings!\n",
    "    non_string_indexes = [i for i in range(len(ingredient[0])) if type(ingredient[0][i][0]) is not str]\n",
    "    # now, move pairwise through the non_string_index values, re-tagging\n",
    "    # whatever's in between those indexes\n",
    "    prev = 0\n",
    "    for i in non_string_indexes:\n",
    "        ingredient[0][prev:i] = nltk.pos_tag([ing[0] for ing in ingredient[0][prev:i]])\n",
    "        prev = i + 1\n",
    "    ingredient[0][prev:] = nltk.pos_tag([ing[0] for ing in ingredient[0][prev:]])\n",
    "    return ingredient\n",
    "\n",
    "    \n",
    "\n",
    "def tag_alt_amounts(tokens: list) -> list:\n",
    "    # tag alt amounts indicated by amounts after amounts, either in parentheses or separated by slashes\n",
    "    out = []\n",
    "    prev_type = None\n",
    "    alt_active = False\n",
    "    multiple_alts = False\n",
    "    for token in tokens:\n",
    "        if prev_type not in [Parser.AMOUNT, Parser.ALT_AMOUNT, Parser.EA_AMOUNT]:\n",
    "            prev_type = token[1]\n",
    "    \n",
    "        elif token[0] in ['(', '[', '/', 'or']:\n",
    "            alt_active = True\n",
    "            multiple_alts = token[0] in ['(', '[']\n",
    "\n",
    "        elif not alt_active:\n",
    "            prev_type = token[1]\n",
    "            \n",
    "        elif token[1] == Parser.AMOUNT:\n",
    "            # retag it as an ALT_AMOUNT incl. ALT_QUANTITY and ALT_UNIT\n",
    "            for i in range(len(token[0])):\n",
    "                if token[0][i][1] == Parser.QUANTITY:\n",
    "                    token[0][i] = (token[0][i][0], Parser.ALT_QUANTITY)\n",
    "                elif token[0][i][1] == Parser.UNIT:\n",
    "                    token[0][i] = (token[0][i][0], Parser.ALT_UNIT)\n",
    "            token[1] = Parser.ALT_AMOUNT\n",
    "                    \n",
    "            if not multiple_alts:\n",
    "                alt_active = False\n",
    "                multiple_alts = False\n",
    "\n",
    "        elif token[0] in [')', ']']:\n",
    "            alt_active = False\n",
    "            multiple_alts = False\n",
    "\n",
    "        out += [token]\n",
    "\n",
    "    return out\n",
    "\n",
    "def tag(tokens: list) -> list:\n",
    "    # this is the main semantic undertaking\n",
    "    data = nltk.pos_tag(tokens)\n",
    "    # print('0:', data, '\\n')\n",
    "    data = tag_units_and_quantities(data)\n",
    "    # print('1:', data, '\\n')\n",
    "    data = coalesce_units(data)\n",
    "    # print('2:', data, '\\n')\n",
    "    data = tag_amounts(data)\n",
    "    # print('3:', data, '\\n')\n",
    "    data = tag_alt_amounts(data)\n",
    "    # print('4:', data, '\\n')\n",
    "    data = complete_amounts(data)\n",
    "    # print('5:', data, '\\n')\n",
    "    data = merge_amounts(data, Parser.ALT_AMOUNT)\n",
    "    # print('6:', data, '\\n')\n",
    "    data = merge_amounts(data, Parser.EA_AMOUNT)\n",
    "    # print('7:', data, '\\n')\n",
    "    data = strip_qualifier_parens(data)\n",
    "    # print('8:', data, '\\n')\n",
    "    data = strip_remnants(data)\n",
    "    # print('strip_remnants:', data, '\\n')\n",
    "    data = plus_amounts(data)\n",
    "    # print('plus_amounts:', data, '\\n')\n",
    "    data = label_ingredients(data)\n",
    "    # re-pos_tag the leftovers, as nltk doesn't do a great job when ingredient data is mixed in,\n",
    "    # and we want the most accurate POS values for the focus() call that follows.\n",
    "    # print('label_ingredients:', data, '\\n')\n",
    "    data = [retag(d) for d in data]\n",
    "    # print('retag:', data, '\\n')\n",
    "    data = [focus(d) for d in data]\n",
    "    # print('focus:', data, '\\n')\n",
    "    data = [unstop(d) for d in data]\n",
    "    # print('unstop:', data, '\\n')\n",
    "    data = [resolve_amounts(d) for d in data]\n",
    "    # print('resolve_amounts:', data, '\\n')\n",
    "    return data\n",
    "\n",
    "def standardize_unit(unit):\n",
    "    from parser import UNITS\n",
    "    if unit in UNITS:\n",
    "        return UNITS[unit]\n",
    "    elif unit[-1] == 's' and unit[:-1] in UNITS:\n",
    "        return UNITS[unit[:-1]]\n",
    "    elif unit[:-2] == 'es' and unit[:-2] in UNITS:\n",
    "        return UNITS[unit[:-2]]\n",
    "\n",
    "\n",
    "def quantify(amount):\n",
    "    \"\"\" Convert a Parser.AMOUNT object into a mass, volume, or `each` quantity,\n",
    "    return a (quantity, unit) tuple. mass's unit is `g`, volume is `ml`,\n",
    "    and each is `each`.\n",
    "\n",
    "    Mass is preferred to volume; volume is preferred to `each`. Parse the amount,\n",
    "    and standardize it to the most preferred option available.\n",
    "    \"\"\"\n",
    "    from convert import Convert\n",
    "    qty, unit = None, None\n",
    "    # print('quantify:', amount)\n",
    "    for el in amount[:2]:\n",
    "        if len(el) > 1:\n",
    "            if el[1] in [Parser.QUANTITY, Parser.EA_QUANTITY, Parser.ALT_QUANTITY]:\n",
    "                qty = float(el[0])\n",
    "            elif el[1] in [Parser.UNIT, Parser.EA_UNIT, Parser.ALT_UNIT]:\n",
    "                unit = el[0]\n",
    "        \n",
    "    unit = standardize_unit(unit)\n",
    "    if unit in Convert.VOLUME:\n",
    "        qty *= Convert.VOLUME[unit]\n",
    "        unit = 'ml'\n",
    "\n",
    "    elif unit in Convert.MASS:\n",
    "        qty *= Convert.MASS[unit]\n",
    "        unit = 'g'\n",
    "\n",
    "    alt_amount = None\n",
    "    plus_amount = None\n",
    "    ea_amount = None\n",
    "    for extra in amount[2:]:\n",
    "        # print(extra)\n",
    "        if extra[1] == Parser.ALT_AMOUNT:\n",
    "            alt_amount = quantify(extra[0])\n",
    "        elif extra[1] == Parser.EA_AMOUNT:\n",
    "            ea_amount = quantify(extra[0])\n",
    "        elif extra[1] == Parser.PLUS_AMOUNT:\n",
    "            plus_amount = quantify(extra[0])\n",
    "\n",
    "    # the order here matters, as we want to convert in reverse\n",
    "    # order of priority, so that, for example, `ea` values get swapped out\n",
    "    # before we assess `g`-specific modifications\n",
    "    if unit in ['ea', 'pkg']:\n",
    "        # print('unit is `ea`')\n",
    "        if ea_amount:\n",
    "            qty *= ea_amount[0]\n",
    "            unit = ea_amount[1]\n",
    "        elif alt_amount and alt_amount[1] in ['g', 'ml']:\n",
    "            qty, unit = alt_amount\n",
    "\n",
    "    if unit == 'ml':\n",
    "        # print('unit is `ml`')\n",
    "        if alt_amount and alt_amount[1] == 'g':\n",
    "            qty, unit = alt_amount\n",
    "        elif plus_amount and plus_amount[1] == 'ml':\n",
    "            qty += plus_amount[0]\n",
    "    \n",
    "    if unit == 'g':\n",
    "        # if we have a mass value, we only care about plus_amounts\n",
    "        # print('unit is `g`; plus_amount is', plus_amount)\n",
    "        if plus_amount and plus_amount[1] == 'g':\n",
    "            # print('adding plus amount')\n",
    "            qty += plus_amount[0]\n",
    "\n",
    "    # else:\n",
    "    #     print('unit is something weird:', unit)\n",
    "\n",
    "    # print('quantified:', qty, unit)\n",
    "    # print('alt_amount:', alt_amount)\n",
    "    # print('ea_amount:', ea_amount)\n",
    "    # print('plus_amount:', plus_amount, '\\n')\n",
    "    return qty, unit\n",
    "\n",
    "\n",
    "def resolve_amounts(ingredient:list) -> list:\n",
    "    resolved = []\n",
    "    # print(ingredient)\n",
    "    for token in ingredient[0]:\n",
    "        if token[1] not in [Parser.AMOUNT, Parser.EA_AMOUNT, Parser.ALT_AMOUNT]:\n",
    "            resolved += [token]\n",
    "            continue\n",
    "        qty, unit = quantify(token[0])\n",
    "        resolved += [[[(qty, Parser.QUANTITY), (unit, Parser.UNIT)], token[1]]]\n",
    "    return [resolved, ingredient[1]]\n",
    "\n",
    "def detokenize(ingredient_tokens):\n",
    "    out = ''\n",
    "    for t in ingredient_tokens:\n",
    "        if out and t[1] not in [',']:\n",
    "            out += ' '\n",
    "        if t[1] in [Parser.AMOUNT]:\n",
    "            out += '{} {}'.format(t[0][0][0], t[0][1][0])\n",
    "            # continue # TODO: reduce it to it's canonical representation\n",
    "        else:\n",
    "            # print(t)\n",
    "            try:\n",
    "                out += t[0]\n",
    "            except Exception as err:\n",
    "                print(t)\n",
    "                raise err\n",
    "    return out\n",
    "        \n",
    "def detokenize_ingredient(ingredient):\n",
    "    # convert tokens to a string (for human readability)\n",
    "    out = ''\n",
    "    # print(ingredient_tokens)\n",
    "    if not ingredient:\n",
    "        return out\n",
    "    for ingred in ingredient:\n",
    "        # print('INGRED::', ingred)\n",
    "        if out:\n",
    "            out += '; OR' if ingred[1] == Parser.ALT_INGREDIENT else ' +'\n",
    "        try:\n",
    "            tokens = ingred[0]\n",
    "        except Exception as err:\n",
    "            print(ingred)\n",
    "            raise err\n",
    "        out += detokenize(tokens)\n",
    "    return out\n",
    "\n",
    "# @timing\n",
    "def parse(raw_text: str):\n",
    "    \"\"\" Extract quantity and name information from an ingredient entry\n",
    "    \"\"\"\n",
    "    text = preprocess(raw_text)\n",
    "    if disregard(text):\n",
    "        return None\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    tokens = decimate(tokens)\n",
    "    tokens = rangeify(tokens)\n",
    "\n",
    "    tagged_data = tag(tokens)\n",
    "    # TODO: figure out what to do about \"for example\", \"like\", \"such as\" phrases\n",
    "    # TODO: teach the matcher to look at the last NN* before the first (if any) comma after the AMT.\n",
    "    # That is almost always your guy, and then you could move back if there were alternatives. We'll need to futz this,\n",
    "    # but I think having the POS structure and the AMT cleanly extracted is a game changer.\n",
    "\n",
    "    # return tagged_data\n",
    "    # return in the format it already knows!\n",
    "    if tagged_data:\n",
    "        tagged_data = tagged_data[0] # only use the first entry for now\n",
    "        data = {\n",
    "            QTYS:[],\n",
    "            NAMES: '',\n",
    "            MODS: [],\n",
    "            STRIPPED_WORDS:[]\n",
    "        }\n",
    "        if tagged_data[0][0][1] == Parser.AMOUNT:\n",
    "            data[QTYS] = [{el[1].lower(): el[0] for el in tagged_data[0][0][0]}]\n",
    "            data[NAMES] = detokenize(tagged_data[0][1:])\n",
    "    else:\n",
    "        data = None\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "from time import time\n",
    "\n",
    "def timing(f):\n",
    "    @wraps(f)\n",
    "    def wrap(*args, **kw):\n",
    "        n = 100\n",
    "        ts = time()\n",
    "        for _ in range(n):\n",
    "            f(*args, **kw)\n",
    "        te = time()\n",
    "        print ('func: {} took: {:2.4f}ms'.format(f.__name__, (te-ts)*1000/n))\n",
    "        result = f(*args, **kw)\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert import Convert\n",
    "import re\n",
    "import logging\n",
    "import nltk\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def case_insensitize(text):\n",
    "    \"\"\" return a regex pattern that matches the input text regardless of casing \"\"\"\n",
    "    out = r''\n",
    "    for char in text.lower():\n",
    "        out += r'[{}{}]'.format(char, char.upper())\n",
    "    return out\n",
    "\n",
    "\n",
    "class Parser:\n",
    "    \"\"\" Parse amount information and ingredient identification text\n",
    "    from raw recipe text.\n",
    "    \"\"\"\n",
    "    # --------- Standard attribute names --------- #\n",
    "    MODS = 'mods'\n",
    "    QTYS = 'qtys'\n",
    "    NAMES = 'names'\n",
    "    STRIPPED_WORDS = 'stripped_words'\n",
    "    EA = 'ea'\n",
    "    UNIT = 'UNIT'\n",
    "    ALT_UNIT = 'ALT_UNIT'\n",
    "    EA_UNIT = 'EA_UNIT'\n",
    "    QUANTITY = 'QTY'\n",
    "    ALT_QUANTITY = 'ALT_QTY'\n",
    "    EA_QUANTITY = 'EA_QTY'\n",
    "    AMOUNT = 'AMT'\n",
    "    ALT_AMOUNT = 'ALT_AMT'\n",
    "    EA_AMOUNT = 'EA_AMT'\n",
    "    PLUS_AMOUNT = 'PLUS_AMT'\n",
    "    INGREDIENT = 'INGRED'\n",
    "    ALT_INGREDIENT = 'ALT_INGRED'\n",
    "\n",
    "    # --------- Key punctuation --------- #\n",
    "    OPEN_PARENTHESES = ['(', '[']\n",
    "    CLOSE_PARENTHESES = [')', ']']\n",
    "\n",
    "    # --------- Conversions and standardizations --------- #\n",
    "    TEXT_NUMBERS = {\n",
    "        'one': 1,\n",
    "        'two': 2,\n",
    "        'three': 3,\n",
    "        'four': 4,\n",
    "        'five': 5,\n",
    "        'six': 6,\n",
    "        'seven': 7,\n",
    "        'eight': 8,\n",
    "        'nine': 9,\n",
    "        'ten': 10,\n",
    "        'dozen': 12,\n",
    "        'half-dozen': 6,\n",
    "        'half dozen': 6,\n",
    "        'half a dozen': 6\n",
    "    }\n",
    "\n",
    "    VULGAR_FRACTIONS = {\n",
    "        u'\\u00bc': '1/4',  # Â¼\n",
    "        u'\\u00bd': '1/2',  # Â½\n",
    "        u'\\u00be': '3/4',  # Â¾\n",
    "        u'\\u2150': '1/7',  # â\n",
    "        u'\\u2151': '1/9',  # ...\n",
    "        u'\\u2152': '1/10',\n",
    "        u'\\u2153': '1/3',\n",
    "        u'\\u2154': '2/3',\n",
    "        u'\\u2155': '1/5',\n",
    "        u'\\u2156': '2/5',\n",
    "        u'\\u2157': '3/5',\n",
    "        u'\\u2158': '4/5',\n",
    "        u'\\u2159': '1/6',\n",
    "        u'\\u215a': '5/6',\n",
    "        u'\\u215b': '1/8',\n",
    "        u'\\u215c': '3/8',\n",
    "        u'\\u215d': '5/8',\n",
    "        u'\\u215e': '7/8',\n",
    "        u'\\u215f': '',\n",
    "        u'\\u2189': '',\n",
    "    }\n",
    "\n",
    "    UNITS = {\n",
    "        'ml': 'ml',\n",
    "        'milliliter': 'ml',\n",
    "        'millilitre': 'ml',\n",
    "        'litre': 'l',\n",
    "        'liter': 'l',\n",
    "        'l': 'l',\n",
    "        'g': 'g',\n",
    "        'gram': 'g',\n",
    "        'kg': 'kg',\n",
    "        'kilo': 'kg',\n",
    "        'kilogram': 'kg',\n",
    "        'kilogramme': 'kg',\n",
    "        'cup': 'cup',\n",
    "        'c': 'cup',\n",
    "        'tablespoon': 'tablespoon',\n",
    "        'tbsp': 'tablespoon',\n",
    "        'teaspoon': 'teaspoon',\n",
    "        'tsp': 'teaspoon',\n",
    "        'T': 'tablespoon',\n",
    "        't': 'teaspoon',\n",
    "        'pound': 'pound',\n",
    "        'lb': 'pound',\n",
    "        'ounce': 'ounce',\n",
    "        'oz': 'ounce',\n",
    "        'quart': 'quart',\n",
    "        'qt': 'quart',\n",
    "        'pint': 'pint',\n",
    "        'pt': 'pint',\n",
    "        'dash': 'dash',\n",
    "        'pinch': 'pinch',\n",
    "        'handful': 'handful',\n",
    "        'fistful': 'fistful',\n",
    "        'smidgen': 'smidgen',\n",
    "        'bunch': 'bunch',\n",
    "        'drop': 'drop',\n",
    "        'ea': 'ea',\n",
    "        'ear': 'ea',\n",
    "        'slice': 'ea',\n",
    "        'stalk': 'ea',\n",
    "        'stick': 'ea',\n",
    "        'sprig': 'ea',\n",
    "        'can': 'pkg',\n",
    "        'tin': 'pkg',\n",
    "        'jar': 'pkg',\n",
    "        'fillet': 'ea',  # currently being stripped because it doesn't appear in the right position to be capture as a quantity\n",
    "    }\n",
    "\n",
    "    # --------- Stop/otherwise unhelpful words --------- #\n",
    "    STOPWORDS = {\n",
    "        # nltk base set (english) with 't', 'with', 'or', 'to', 'in', 'at', 'on', 'of' removed\n",
    "        'had', 'few', 'under', 'an', 'its', 'why', 'were', 'all', 'doing',\n",
    "        'while', 'how', 'don', 'same', 'is', 'because', 'him', 'ourselves', 'off',\n",
    "        'herself', 'has', 'into', 'd', 'out', 'he', 'against', 'themselves',\n",
    "        'wouldn', 'theirs', 'be', 'above', 'up', 'own', 'are', 'when',\n",
    "        'through', 'will', 'by', 'our', 'who', 'between', 'so', 'ain', 'this',\n",
    "        'than', 'aren', 'them', 'not', 'wasn', 'your', 'these', 'himself',  # 'of',\n",
    "        'down', 'won', 'for', 'only', 'as', 'myself', 'both', 'yours', 'during',\n",
    "        'you', 'too', 'where', 's', 'hadn', 'about', 'and', 'been', 'very', 'do',\n",
    "        'over', 'most', 'o', 'that', 'was', 'again', 'further',\n",
    "        'couldn', 'having', 'hasn', 'mightn', 'me', 'no', 'her', 'hers', 'ours',\n",
    "        'haven', 'my', 'it', 'nor', 'those', 'she', 'what', 'a', 're',\n",
    "        'but', 'just', 'once', 'whom', 'from', 'am', 'below', 'mustn', 'ma', 've',\n",
    "        'the', 'more', 'll', 'didn', 'needn', 'then', 'isn', 'should', 'his',\n",
    "        'before', 'doesn', 'm', 'did', 'yourself', 'other', 'yourselves',\n",
    "        'itself', 'any', 'being', 'i', 'here', 'some', 'which', 'we', 'such',\n",
    "        'there', 'weren', 'if', 'now', 'shan', 'after', 'they', 'shouldn', 'have',\n",
    "        'their', 'y', 'does', 'until'\n",
    "    }\n",
    "\n",
    "    STOPWORDS |= {\n",
    "        '',  # strip empty words (e.g. punctuation replacements)\n",
    "        '~',\n",
    "        'amount',\n",
    "        'approx',\n",
    "        'approximately',\n",
    "        'approx.',\n",
    "        'appr.',\n",
    "        'assorted',\n",
    "        'baby',\n",
    "        'coarse',\n",
    "        'coarsely',\n",
    "        'cold',\n",
    "        'cooled',\n",
    "        'cored',\n",
    "        'cut',\n",
    "        'desired',\n",
    "        'drizzling',\n",
    "        'equal',\n",
    "        'fillet',\n",
    "        'fillets',\n",
    "        'fine',\n",
    "        'firm',\n",
    "        'finely',\n",
    "        'flaky',\n",
    "        'fresh',  # leave this in for \"fresh bean\"?\n",
    "        'freshly',\n",
    "        'frozen',  # leave this in for \"frozen pea\"?\n",
    "        'garnish',\n",
    "        'garnishes',\n",
    "        'gently',\n",
    "        'grilled',\n",
    "        'halved',\n",
    "        'halves',\n",
    "        'high-quality',\n",
    "        'hulled',\n",
    "        'interval',\n",
    "        'kitchen',\n",
    "        'like',\n",
    "        'twine',\n",
    "        'least',\n",
    "        'leftover',\n",
    "        'low-sodium',\n",
    "        'medium',\n",
    "        'mild',\n",
    "        'optional',\n",
    "        'organic',\n",
    "        'peeled',\n",
    "        'picked',\n",
    "        'pitted',\n",
    "        'plain',\n",
    "        'preferably',\n",
    "        'pure',\n",
    "        'quartered',\n",
    "        'ripe',\n",
    "        'room',\n",
    "        'roughly',\n",
    "        'salted',\n",
    "        'scrubbed',\n",
    "        'seeded',\n",
    "        'serving',\n",
    "        'shredded',\n",
    "        'similar',\n",
    "        'softened',\n",
    "        'stemmed',\n",
    "        'store-bought',\n",
    "        'taste',\n",
    "        'temperature',\n",
    "        'thawed',\n",
    "        'thinly',\n",
    "        'toasted',\n",
    "        'trimmed',\n",
    "        'unsalted',\n",
    "        'unseasoned',\n",
    "        'unsweetened',\n",
    "        'washed',\n",
    "    }\n",
    "\n",
    "    PREP_MODS = {  # try to suss out preps that (might) affect density\n",
    "        'beaten',\n",
    "        'boned',\n",
    "        'boneless',\n",
    "        'canned',\n",
    "        'chopped',\n",
    "        'cleaned',\n",
    "        'creamed',\n",
    "        'crosswise',\n",
    "        'crumbled',\n",
    "        'crushed',\n",
    "        'dice',\n",
    "        'diced',\n",
    "        'dissolved',\n",
    "        'drained',\n",
    "        'flaked',\n",
    "        'freeze-dried',\n",
    "        'generous',\n",
    "        'grated',\n",
    "        'ground',\n",
    "        'gutted',\n",
    "        'heaping',\n",
    "        'jarred',\n",
    "        'large',  # can be used to modify a quantity, e.g \"large handful\"\n",
    "        'lengthwise',\n",
    "        'lightly',\n",
    "        'loosely',\n",
    "        'melted',\n",
    "        'minced',\n",
    "        'packed',\n",
    "        'pressed',\n",
    "        'puree',\n",
    "        'pureed',\n",
    "        'roasted',\n",
    "        'sauteed',\n",
    "        'scaled',\n",
    "        'shelled',\n",
    "        'sifted',\n",
    "        'skin-on',\n",
    "        'skinless',\n",
    "        'skinned',\n",
    "        'sliced',\n",
    "        'small',  # can be used to modify a quantity, e.g \"small handful\"\n",
    "        'smashed',\n",
    "        'smoked',\n",
    "        'squeezed',\n",
    "        'steamed',\n",
    "        'tightly,'\n",
    "        'torn',\n",
    "        'well',\n",
    "        'whipped',\n",
    "        'whisked',\n",
    "        'zested',\n",
    "    }\n",
    "\n",
    "    # --------- Unit regexes --------- #\n",
    "    RE_UNIT_LABELS = [case_insensitize(unit) for unit in UNITS]\n",
    "    RE_UNITS_GROUP = r'|'.join(RE_UNIT_LABELS)\n",
    "    RE_UNITS_PLURAL = re.compile(r'({})([sei]*)$'.format(RE_UNITS_GROUP))\n",
    "\n",
    "    # --------- Input text pre-processing --------- #\n",
    "    @classmethod\n",
    "    def preprocess(cls, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Preprocess/standardize ingredient text in preparation for analysis:\n",
    "            * Strip leading/trailing whitespace\n",
    "            * Strip out any html tags\n",
    "            * Expand vulgar fractions, prepending a space, e.g. \"1Â¼\" -> \"1 1/4\" (nltk takes 1/4 as a number, huzzah!)\n",
    "            * Put a space between spaceless units, e.g. \"6c\" -> \"6 c\"\n",
    "        \"\"\"\n",
    "        text = text.strip()  # remove leading/trailing whitespace\n",
    "        text = cls.strip_html_tags(text)\n",
    "        text = cls.devulgarize(text)\n",
    "        text = cls.gap_units(text)\n",
    "        if not cls.disregard(text):\n",
    "            return text\n",
    "\n",
    "    RE_GAPLESS_UNITS = re.compile(r'(\\d)({})\\b'.format(RE_UNITS_GROUP))\n",
    "\n",
    "    @classmethod\n",
    "    def gap_units(cls, text: str) -> str:\n",
    "        \"\"\" Insert a space between quantities and units, e.g. '6c' -> '6 c' \"\"\"\n",
    "        return re.sub(cls.RE_GAPLESS_UNITS, r'\\g<1> \\g<2>', text)\n",
    "\n",
    "    @classmethod\n",
    "    def devulgarize(cls, text):\n",
    "        \"\"\" Expand unicode vulgar fractions, prepending a space\"\"\"\n",
    "        for k, v in cls.VULGAR_FRACTIONS.items():\n",
    "            text = text.replace(k, ' ' + v)\n",
    "        return text\n",
    "\n",
    "    # strip_html_tags regexes\n",
    "    RE_TAG_START = re.compile(r'<((a)|(strong)|(span)).*?>')\n",
    "    RE_TAG_END = re.compile(r'</((a)|(strong)|(span))>')\n",
    "\n",
    "    @classmethod\n",
    "    def strip_html_tags(cls, text):\n",
    "        # At some point, we need to decide the \"correct\" way to handle inlined\n",
    "        # recipe links/whether we want to do anything about that\n",
    "        return re.sub(cls.RE_TAG_END, '', re.sub(cls.RE_TAG_START, '', text))\n",
    "\n",
    "    # disregard regexes\n",
    "    RE_ALL_PARENS = re.compile(r'^\\([^\\)]*\\)$')\n",
    "    RE_COLON_ANYWHERE = re.compile(r'.*:.*')\n",
    "    RE_DISREGARD_HEADERS = [re.compile(r) for r in [\n",
    "        r'accompaniments?:',\n",
    "        r'equipment:',\n",
    "        r'for .*:',\n",
    "        r'garnish(es)?:',\n",
    "        r'glass(ware)?:',\n",
    "        r'grill heat:',\n",
    "        r'ingredient info:',\n",
    "        r'note:',\n",
    "        r'serving suggestion(s)?:',\n",
    "        r'special equipment:',\n",
    "        r'test-kitchen tip:',\n",
    "        r'type of fire:',\n",
    "    ]]\n",
    "\n",
    "    @classmethod\n",
    "    def disregard(cls, text: str) -> bool:\n",
    "        \"\"\" Return True if text isn't a \"real\" ingredient\n",
    "        Things that are marked optional remain TBD, but I lean towards disregarding these, too.\n",
    "        \"\"\"\n",
    "\n",
    "        if not text:  # empty\n",
    "            return True\n",
    "        if text.endswith(':'):\n",
    "            # Ends in a colon \":\" almost always means it's a directive.\n",
    "            # There are a few cases where a quantity is provided as part of this,\n",
    "            # but this is very rare, and more often than not the quantity is redundant\n",
    "            # with subsequent entries. We'll live with the misses here.\n",
    "            return True\n",
    "\n",
    "        if re.match(cls.RE_ALL_PARENS, text):\n",
    "            # entirely parenthetical, e.g. \"(Essential oil complement: orange)\"\n",
    "            return True\n",
    "\n",
    "        if re.match(cls.RE_COLON_ANYWHERE, text):\n",
    "            # check for certain other directives, which typically include a colon,\n",
    "            # e.g. \"Equipment:...\", \"Accompaniment:...\", \"Ingredient info:...\"\n",
    "            lowered = text.lower()\n",
    "            # re.match only considers string start, which is what we want\n",
    "            if any([re.match(pttrn, lowered) for pttrn in cls.RE_DISREGARD_HEADERS]):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    # --------- Tokenization --------- #\n",
    "    @classmethod\n",
    "    def tokenize(cls, text: str) -> list:\n",
    "        \"\"\" A wrapper on nltk.word_tokenize, except when\n",
    "        we see things like \"grams/3\" in the nltk tokens (e.g. from an ingredient that read\n",
    "        \"45 grams/3 ounces of oil\"), replace it with \"grams\", \"/\", \"3\" so we can use the\n",
    "        slash as an indicator of an alternative measure.\n",
    "        \"\"\"\n",
    "        # tokenize and tag the text usin nltk defaults\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        out = []\n",
    "        for t in tokens:\n",
    "            if '/' not in t:\n",
    "                out += [t]\n",
    "                continue\n",
    "\n",
    "            parts = t.split('/')\n",
    "            digits = len(\n",
    "                [el for el in parts if el[:1].isdigit() or el[-1:].isdigit()])\n",
    "            if digits == len(parts):\n",
    "                out += [t]\n",
    "                continue\n",
    "\n",
    "            for i, p in enumerate(parts):\n",
    "                if p:  # don't include empty strings\n",
    "                    out += [p]\n",
    "                if i < len(parts) - 1:\n",
    "                    out += ['/']\n",
    "\n",
    "        out = [cls.numerify(el) for el in out]\n",
    "        out = [cls.separate_qualifier_units(el) for el in out]\n",
    "        out = [e for el in out for e in el]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def separate_qualifier_units(cls, text: str) -> list:\n",
    "        tokens = text.split('-')\n",
    "        if len(tokens) == 1:\n",
    "            return [text]\n",
    "\n",
    "        if re.match(cls.RE_UNITS_PLURAL, tokens[-1]):\n",
    "            unit = tokens[-1]\n",
    "            tokens[-1] = ''\n",
    "            return '-'.join(tokens), unit\n",
    "\n",
    "        return [text]\n",
    "\n",
    "    @classmethod\n",
    "    def numerify(cls, text: str) -> list:\n",
    "        \"\"\" Convert text numbers to digits \"\"\"\n",
    "        tokens = text.split('-')\n",
    "\n",
    "        if len(tokens) == 1:\n",
    "            # no dashes present; tokens[0] == text\n",
    "            return str(cls.TEXT_NUMBERS.get(text.lower(), text))\n",
    "        elif not any([token.lower() in cls.TEXT_NUMBERS for token in tokens]):\n",
    "            # no numbers\n",
    "            return text\n",
    "\n",
    "        # We have numeric text, which should only be converted to digits if\n",
    "        # it's being used as a quantity (vs, most notably, five-spice powder)\n",
    "        # or a range of quantities.\n",
    "        # This means the token after a number must be either a unit, a range\n",
    "        # indicator (to, or), or empty before we will convert it\n",
    "\n",
    "        if len(tokens) == 2:\n",
    "            if (not tokens[1]) or re.match(cls.RE_UNITS_PLURAL, tokens[1]):\n",
    "                # e.g. \"five-\", \"six-ounce\"\n",
    "                tokens[0] = cls.TEXT_NUMBERS.get(tokens[0].lower(), tokens[0])\n",
    "\n",
    "        elif len(tokens) == 4 and re.match(cls.RE_UNITS_PLURAL, tokens[-1]) and tokens[1].lower() in ['or', 'to']:\n",
    "            # e.g. \"five-to-six-ounce\"\n",
    "            tokens = [cls.TEXT_NUMBERS.get(\n",
    "                token.lower(), token) for token in tokens]\n",
    "\n",
    "        return '-'.join([str(t) for t in tokens])\n",
    "\n",
    "    # --------- Number unification --------- #\n",
    "    @classmethod\n",
    "    def decimate(cls, tokens: list) -> list:\n",
    "        \"\"\" Convert fractions to decimals.\n",
    "        Yes, the method name is cheeky.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        prev_digit = None\n",
    "        for t in tokens:\n",
    "            dash = ''\n",
    "            if t.endswith('-'):\n",
    "                t = t[:-1]\n",
    "                dash = '-'\n",
    "\n",
    "            parts = t.split('/')\n",
    "            if len(parts) != 2:\n",
    "                out += [t+dash]\n",
    "\n",
    "            elif all([p.isdigit() for p in parts]):\n",
    "                # `1/2`\n",
    "                val = float(parts[0])/float(parts[1])\n",
    "                if prev_digit:\n",
    "                    # sum and overwrite\n",
    "                    val += prev_digit\n",
    "                    out[-1] = str(val) + dash\n",
    "                else:\n",
    "                    out += [str(val) + dash]\n",
    "\n",
    "            elif parts[1].isdigit() and len(parts[0].split('-')) == 2 and all([p.isdigit() for p in parts[0].split('-')]):\n",
    "                # `1-1/2`\n",
    "                denominator = float(parts[1])\n",
    "                whole, numerator = parts[0].split('-')\n",
    "                val = float(whole) + float(numerator)/denominator\n",
    "                out += [str(val) + dash]\n",
    "\n",
    "            else:\n",
    "                out += [t+dash]\n",
    "\n",
    "            prev_digit = float(t) if t.isdigit() else None\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def rangeify(cls, tokens: list) -> list:\n",
    "        \"\"\"\n",
    "        Convert ranged numbers to their midpoints, e.g. `6 to 8` -> `7`.\n",
    "\n",
    "        This does *not* range units, e.g. it excludes things like \n",
    "        `6 cups to 2 gallons` which will be done elsewhere TBD. \n",
    "\n",
    "        Trailing dashes are preserved.\n",
    "\n",
    "        Covered formats:\n",
    "            `6 to 8`\n",
    "            `6-to-8`\n",
    "            `6-to-8-`\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        prev_value = None\n",
    "        range_active = False\n",
    "        for token in tokens:\n",
    "            # look for numbers, numbers ending in dashes\n",
    "            dash = ''\n",
    "            if token[-1] == '-':\n",
    "                dash = '-'\n",
    "                token = token[:-1]\n",
    "\n",
    "            if any([cc in token for cc in ['-to-', '-or-']]):\n",
    "                cc = '-to-' if '-to-' in token else '-or-'\n",
    "                parts = token.split(cc)\n",
    "                try:\n",
    "                    midpoint = (float(parts[0]) + float(parts[1])) / 2\n",
    "                    out += [str(midpoint) + dash]\n",
    "                except ValueError:\n",
    "                    out += [token + dash]\n",
    "                range_active = False\n",
    "                prev_value = None\n",
    "\n",
    "            elif token in ['or', 'to']:\n",
    "                range_active = bool(prev_value)\n",
    "                out += [token + dash]\n",
    "\n",
    "            else:  # see if it's a number\n",
    "                try:\n",
    "                    val = float(token)\n",
    "                    if range_active:\n",
    "                        midpoint = (val + prev_value) / 2\n",
    "                        out[-2:] = [str(midpoint) + dash]\n",
    "                        prev_value = None\n",
    "                    else:\n",
    "                        out += [str(val) + dash]\n",
    "                        prev_value = val\n",
    "                except ValueError:\n",
    "                    out += [token + dash]\n",
    "                    prev_value = None\n",
    "                range_active = False\n",
    "\n",
    "        return out\n",
    "\n",
    "    # --------- Ingredient tagging --------- #\n",
    "    @classmethod\n",
    "    def tag(cls, tokens: list) -> list:\n",
    "        \"\"\"\n",
    "        This is the main semantic undertaking of the Parser. A series of steps\n",
    "        build on each other to extract the amount information, and consolidate\n",
    "        the remaining text to focus on useful words and phrases.\n",
    "        \"\"\"\n",
    "        data = nltk.pos_tag(tokens)\n",
    "        data = cls.tag_units_and_quantities(data)\n",
    "        data = cls.coalesce_units(data)\n",
    "        data = cls.tag_amounts(data)\n",
    "        data = cls.tag_alt_amounts(data)\n",
    "        data = cls.complete_amounts(data)\n",
    "        data = cls.merge_amounts(data, cls.ALT_AMOUNT)\n",
    "        data = cls.merge_amounts(data, cls.EA_AMOUNT)\n",
    "        data = cls.strip_qualifier_parens(data)\n",
    "        data = cls.strip_remnants(data)\n",
    "        data = cls.plus_amounts(data)\n",
    "        # hereafter, ingredients is a list of separate ingredients\n",
    "        ingredients = cls.label_ingredients(data)\n",
    "        # re-pos_tag the leftovers, as nltk doesn't do a great job when ingredient data is mixed in,\n",
    "        # and we want the most accurate POS values for the focus() call that follows.\n",
    "        ingredients = [cls.retag(i) for i in ingredients]\n",
    "        ingredients = [cls.focus(i) for i in ingredients]\n",
    "        ingredients = [cls.unstop(i) for i in ingredients]\n",
    "        ingredients = [cls.resolve_amounts(i) for i in ingredients]\n",
    "        print(ingredients)\n",
    "        return ingredients\n",
    "\n",
    "    @classmethod\n",
    "    def tag_units_and_quantities(cls, tokens: list) -> list:\n",
    "        # tag digit (CD) tokens with the QTY type\n",
    "        # tag units tokens with the UNIT type\n",
    "        out = []\n",
    "        for token in tokens:\n",
    "            if token[1] == 'CD':  # digit\n",
    "                try:  # make sure it's actually a digit\n",
    "                    float(token[0])\n",
    "                    out += [(token[0], cls.QUANTITY)]\n",
    "                except ValueError:\n",
    "                    numeric = ''.join(\n",
    "                        [s for s in token[0] if s in '1234567890.'])\n",
    "                    try:\n",
    "                        float(numeric)\n",
    "                        out += [(numeric, cls.QUANTITY)]\n",
    "                    except ValueError:\n",
    "                        out += [token]\n",
    "            elif re.match(cls.RE_UNITS_PLURAL, token[0]):\n",
    "                out += [(token[0], cls.UNIT)]\n",
    "            else:\n",
    "                out += [token]\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def coalesce_units(cls, tokens: list) -> list:\n",
    "        # replace [UNIT]/[UNIT]/*, [UNIT] or [UNIT], and [UNIT], [UNIT] with first unit\n",
    "        out = []\n",
    "        prev_unit = False\n",
    "        for token in tokens:\n",
    "            if token[1] == cls.UNIT:\n",
    "                if prev_unit:\n",
    "                    trunc = False\n",
    "                    while out[-1][0] in ['/', 'or', ',']:\n",
    "                        out = out[:-1]\n",
    "                        trunc = True\n",
    "                    if trunc:\n",
    "                        continue\n",
    "                else:\n",
    "                    prev_unit = True\n",
    "            elif prev_unit and token[0] not in ['/', 'or', ',']:\n",
    "                prev_unit = False\n",
    "            out += [token]\n",
    "        return out\n",
    "\n",
    "    RE_QQR = re.compile(r'[\\d\\.]+-')\n",
    "\n",
    "    @classmethod\n",
    "    def tag_amounts(cls, tokens: list) -> list:\n",
    "        # tag quantities that are qualifiers, as identified by a trailing dash,\n",
    "        # tag their subsequent units and EA_UNITS, and aggregate them with\n",
    "        # their units into a EA_AMOUNT. tag non-qualifier amounts as AMOUNT.\n",
    "        out = []\n",
    "        approximators = ['about', 'approx', 'approx.', 'approximately']\n",
    "        skip = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if re.match(cls.RE_QQR, token[0]):\n",
    "                # drop the trailing dash and mark as an EA_QUANTITY\n",
    "                out += [(token[0][:-1], cls.EA_QUANTITY)]\n",
    "            elif token[1] == cls.UNIT and i > 0 and out[-1][1] == cls.QUANTITY:\n",
    "                if len(tokens) > i + 1 and tokens[i+1][0][:4] in ['each']:\n",
    "                    out[-1] = [[(out[-1][0], cls.EA_QUANTITY),\n",
    "                                (token[0], cls.EA_UNIT)], cls.EA_AMOUNT]\n",
    "                    # also check for a preceding paired amount and convert to an EA_AMOUNT\n",
    "                    # this may not cover all possible cases!\n",
    "                    if len(out) > 2 and out[-2][0] in ['/', 'or'] and out[-3][1] == cls.AMOUNT:\n",
    "                        # mark its paired amount as an EA_AMOUNT\n",
    "                        out[-3] = [out[-3][0], cls.EA_AMOUNT]\n",
    "                    # look for any preceding approximator and remove it\n",
    "                    if len(out) > 1 and out[-2][0] in approximators:\n",
    "                        out = out[:-3] + [out[-1]]\n",
    "                    skip = True  # skip the following 'each'\n",
    "                elif len(out) > 2 and out[-2][0] in approximators:\n",
    "                    # check for 'about' or 'approx*' before this, which indicates an ALT_AMOUNT\n",
    "                    out[-2] = [[(out[-1][0], cls.ALT_QUANTITY),\n",
    "                                (token[0], cls.ALT_UNIT)], cls.ALT_AMOUNT]\n",
    "                    out = out[:-1]\n",
    "                else:\n",
    "                    out[-1] = [[out[-1], token], cls.AMOUNT]\n",
    "            elif token[1] == cls.UNIT and i > 0 and out[-1][1] == cls.EA_QUANTITY:\n",
    "                out[-1] = [[out[-1], (token[0], cls.EA_UNIT)], cls.EA_AMOUNT]\n",
    "            else:\n",
    "                out += [token]\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def tag_alt_amounts(cls, tokens: list) -> list:\n",
    "        # Tag alt amounts as indicated by amounts following other amounts,\n",
    "        # either in parentheses or separated by slashes.\n",
    "        out = []\n",
    "        prev_type = None\n",
    "        alt_active = False\n",
    "        multiple_alts = False\n",
    "        for token in tokens:\n",
    "            if prev_type not in [cls.AMOUNT, cls.ALT_AMOUNT, cls.EA_AMOUNT]:\n",
    "                prev_type = token[1]\n",
    "\n",
    "            elif token[0] in ['(', '[', '/', 'or']:\n",
    "                alt_active = True\n",
    "                multiple_alts = token[0] in ['(', '[']\n",
    "\n",
    "            elif not alt_active:\n",
    "                prev_type = token[1]\n",
    "\n",
    "            elif token[1] == cls.AMOUNT:\n",
    "                # retag it as an ALT_AMOUNT incl. ALT_QUANTITY and ALT_UNIT\n",
    "                for i in range(len(token[0])):\n",
    "                    if token[0][i][1] == cls.QUANTITY:\n",
    "                        token[0][i] = (token[0][i][0], cls.ALT_QUANTITY)\n",
    "                    elif token[0][i][1] == cls.UNIT:\n",
    "                        token[0][i] = (token[0][i][0], cls.ALT_UNIT)\n",
    "                token[1] = cls.ALT_AMOUNT\n",
    "\n",
    "                if not multiple_alts:\n",
    "                    alt_active = False\n",
    "                    multiple_alts = False\n",
    "\n",
    "            elif token[0] in [')', ']']:\n",
    "                alt_active = False\n",
    "                multiple_alts = False\n",
    "\n",
    "            out += [token]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def complete_amounts(cls, tokens: list) -> list:\n",
    "        \"\"\" Pull QTY or UNIT values back to orphaned partners, filling in the\n",
    "        blanks if none are found. Do NOT complete parenthetical orphaned QTYs,\n",
    "        as this is usually not actual quantity information.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        qty_index = None\n",
    "        unit_index = None\n",
    "        any_qty = False\n",
    "        any_unit = False\n",
    "        any_amt = False\n",
    "        parenthetical = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token[0] in cls.OPEN_PARENTHESES:\n",
    "                parenthetical = True\n",
    "            elif parenthetical and token[0] in cls.CLOSE_PARENTHESES:\n",
    "                parenthetical = False\n",
    "            if token[1] == cls.UNIT:\n",
    "                any_unit = True\n",
    "                if qty_index is not None:\n",
    "                    # this is the unit for the existing qty\n",
    "                    out[qty_index] = [[out[qty_index], token], cls.AMOUNT]\n",
    "                    qty_index = None\n",
    "                    continue\n",
    "                elif unit_index is not None:\n",
    "                    # default to qty 1 for existing unit_index\n",
    "                    out[unit_index] = [\n",
    "                        [('1', cls.QUANTITY), out[unit_index]], cls.AMOUNT]\n",
    "                unit_index = len(out)\n",
    "                # out += [token]\n",
    "            elif token[1] == cls.QUANTITY and not parenthetical:\n",
    "                any_qty = True\n",
    "                if qty_index is not None:\n",
    "                    # existing qty is an orphan; default unit to `ea`\n",
    "                    out[qty_index] = [\n",
    "                        [out[qty_index], (cls.EA, cls.UNIT)], cls.AMOUNT]\n",
    "                qty_index = len(out)\n",
    "                # out += [token]\n",
    "            elif token[1] == cls.AMOUNT:\n",
    "                any_amt = True\n",
    "            out += [token]\n",
    "\n",
    "        # clean up any units/qtys left hanging at the end\n",
    "        if qty_index is not None:\n",
    "            out[qty_index] = [\n",
    "                [out[qty_index], (cls.EA, cls.UNIT)], cls.AMOUNT]\n",
    "\n",
    "        if unit_index is not None:\n",
    "            out[unit_index] = [\n",
    "                [('1', cls.QUANTITY), out[unit_index]], cls.AMOUNT]\n",
    "            if unit_index > 0 and out[unit_index-1][1] == cls.EA_AMOUNT:\n",
    "                # if hanging unit was preceded by an EA_AMOUNT, flip the AMOUNT before\n",
    "                # the EA_AMOUNT\n",
    "                out[unit_index-1:unit_index +\n",
    "                    1] = [out[unit_index], out[unit_index-1]]\n",
    "\n",
    "        if not (any_amt or any_unit or any_qty):\n",
    "            out = [[[('1', cls.QUANTITY), (cls.EA, cls.UNIT)],\n",
    "                    cls.AMOUNT]] + out\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def merge_amounts(cls, tokens: list, merge_flavor) -> list:\n",
    "        # link ALT_AMOUNTs to their parent AMOUNT or EA_AMOUNT,\n",
    "        # and link EA_AMOUNTS to their parent AMOUNT\n",
    "        out = []\n",
    "        prev_par_index = None\n",
    "\n",
    "        for token in tokens:\n",
    "            if not token[1] == merge_flavor:\n",
    "                out += [token]\n",
    "\n",
    "            if token[1] == merge_flavor:\n",
    "                if prev_par_index is not None:\n",
    "                    out[prev_par_index][0] += [token]\n",
    "                else:\n",
    "                    out += [token]\n",
    "            elif token[1] in [cls.AMOUNT, cls.EA_AMOUNT]:\n",
    "                # order of this and previous elif matter because EA_AMOUNT\n",
    "                # cannot be a parent if it's the merge_flavor\n",
    "                prev_par_index = len(out) - 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def strip_qualifier_parens(cls, tokens: list) -> list:\n",
    "        # Remove parentheticals, preserving only qualifier amount(s).\n",
    "        # if the parenthetical starts with \"or AMT\", remove the parentheses but\n",
    "        # otherwise preserve it\n",
    "        paren_indexes = []\n",
    "        cur_paren_index = []\n",
    "        parens_only = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token[0] in cls.OPEN_PARENTHESES:\n",
    "                cur_paren_index += [i]\n",
    "                if len(tokens) > i + 2:\n",
    "                    if type(tokens[i+1][0]) is str and tokens[i+1][0].lower() in ['or']:\n",
    "                        if tokens[i+2][1] in [cls.AMOUNT, cls.ALT_UNIT, cls.EA_AMOUNT]:\n",
    "                            parens_only = True\n",
    "            elif token[0] in cls.CLOSE_PARENTHESES:\n",
    "                if cur_paren_index:\n",
    "                    cur_paren_index += [i]\n",
    "                    paren_indexes += cur_paren_index\n",
    "                    cur_paren_index = []  # reset\n",
    "                    parens_only = False\n",
    "            elif cur_paren_index and token[1] not in [cls.EA_AMOUNT, cls.ALT_AMOUNT, cls.AMOUNT] and not parens_only:\n",
    "                cur_paren_index += [i]\n",
    "\n",
    "        return [t for i, t in enumerate(tokens) if i not in paren_indexes]\n",
    "\n",
    "    @classmethod\n",
    "    def strip_remnants(cls, tokens: list) -> list:\n",
    "        # Remove leftover things like slashes that separated amounts.\n",
    "        # Currently looks for slashes and 'of'\n",
    "        out = []\n",
    "        prev_flavor = None\n",
    "\n",
    "        for token in tokens:\n",
    "            if token[0] in ['/', 'of'] and prev_flavor in [cls.AMOUNT, cls.EA_AMOUNT, cls.ALT_AMOUNT]:\n",
    "                continue\n",
    "            out += [token]\n",
    "            prev_flavor = token[1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def plus_amounts(cls, tokens: list) -> list:\n",
    "        # If we find the pattern [AMT, 'plus', AMT], append the second\n",
    "        # AMT to the first one and change its type to Parser.PLUS_AMOUNT\n",
    "        out = []\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token[1] in [cls.AMOUNT]:\n",
    "                # handle this via lookbehind\n",
    "                if i > 1:\n",
    "                    prev_token = tokens[i-1][0]\n",
    "                    if type(prev_token) is str and prev_token.lower() in ['plus']:\n",
    "                        prev_prev_token = tokens[i-2]\n",
    "                        if prev_prev_token[1] in [cls.AMOUNT]:\n",
    "                            out[-2][0] += [(token[0], cls.PLUS_AMOUNT)]\n",
    "                            out = out[:-1]\n",
    "                            continue\n",
    "            out += [token]\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def label_ingredients(cls, tokens: list) -> list:\n",
    "        # Sometimes an ingredient line contains two separate ingredients,\n",
    "        # usually because the second one is provided as an alternative.\n",
    "        # Here, we wrap ingredients in either Parser.INGREDIENT, or Parser.ALT_INGREDIENT\n",
    "        # tags if full ingredients are separated by an 'or' token.\n",
    "        # We also want to pull amounts to the front of ingredients if they don't already\n",
    "        # have one.\n",
    "        # This has quite limited capabilities and will need to be evolved.\n",
    "        ingred = []\n",
    "        ingred_amt = False\n",
    "        ingred_name = False\n",
    "        ingred_has_lead_amt = False\n",
    "        or_index = None\n",
    "        out = []\n",
    "        for token in tokens:\n",
    "            if token[1] in [cls.AMOUNT, cls.ALT_AMOUNT, cls.EA_AMOUNT]:\n",
    "                if ingred_amt and ingred_name and (or_index is not None):\n",
    "                    # we've already parsed a full ingredient; start a new one\n",
    "                    if out:\n",
    "                        out += [[ingred, cls.ALT_INGREDIENT]]\n",
    "                    else:\n",
    "                        del ingred[or_index]\n",
    "                        out += [[ingred, cls.INGREDIENT]]\n",
    "                    ingred = []  # reset\n",
    "                    ingred_name = False\n",
    "                    or_index = None\n",
    "                    ingred_has_lead_amt = False\n",
    "                ingred_amt = True\n",
    "            elif type(token[1]) is str:\n",
    "                if token[1].startswith('NN'):\n",
    "                    ingred_name = True\n",
    "                elif token[0].lower() == 'or' and ingred_name and ingred_amt:\n",
    "                    # only count this if we've seen the rest of the ingredient\n",
    "                    or_index = len(ingred)\n",
    "\n",
    "            if token[1] in [cls.AMOUNT, cls.ALT_AMOUNT, cls.EA_AMOUNT] and not ingred_has_lead_amt:\n",
    "                # move amount to the front only if the ingred doesn't already have an amount\n",
    "                ingred = [token] + ingred  # prepend\n",
    "                ingred_has_lead_amt = True\n",
    "            else:\n",
    "                ingred += [token]\n",
    "\n",
    "        if ingred:\n",
    "            flavor = cls.ALT_INGREDIENT if out else cls.INGREDIENT\n",
    "            out += [[ingred, flavor]]\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def retag(cls, ingredient: list) -> list:\n",
    "        \"\"\" \n",
    "        Retag the remaining words once the amount info has been removed, \n",
    "        parsing the biggest chunks possible to maximize the info given to nltk.\n",
    "        \"\"\"\n",
    "        # Find any ingredient bits that aren't strings!\n",
    "        non_string_indexes = [i for i in range(\n",
    "            len(ingredient[0])) if type(ingredient[0][i][0]) is not str]\n",
    "        # now, move pairwise through the non_string_index values, re-tagging\n",
    "        # whatever's in between those indexes\n",
    "        prev = 0\n",
    "        for i in non_string_indexes:\n",
    "            ingredient[0][prev:i] = nltk.pos_tag(\n",
    "                [ing[0] for ing in ingredient[0][prev:i]])\n",
    "            prev = i + 1\n",
    "        ingredient[0][prev:] = nltk.pos_tag(\n",
    "            [ing[0] for ing in ingredient[0][prev:]])\n",
    "        return ingredient\n",
    "\n",
    "    @classmethod\n",
    "    def focus(cls, ingredient: list) -> list:\n",
    "        \"\"\"\n",
    "        Remove extraneous things like prep mods and unhelpful descriptions.\n",
    "        This is mostly done using parts of speech and their position in the ingredient.\n",
    "        We call this recursively as we eliminate unhelpful bits.\n",
    "        \"\"\"\n",
    "        ingred = ingredient[0]\n",
    "        pos_distractions = ['VBD', 'VBN', 'IN', 'DT']\n",
    "        distractions = ['preferably']\n",
    "\n",
    "        if ingred[-1][0] in [',', ';']:\n",
    "            # eliminate dangling punctuation\n",
    "            ingred = ingred[:-1]\n",
    "            return cls.focus([ingred, ingredient[1]])\n",
    "\n",
    "        first_noun = [i for i in range(len(ingred)) if type(\n",
    "            ingred[i][1]) is str and ingred[i][1].startswith('NN')]\n",
    "        first_noun = min(first_noun) if first_noun else 0\n",
    "\n",
    "        # find the comma indexes, which often delineate phrases\n",
    "        commas = [i for i in range(len(ingred)) if ingred[i][0] in [\n",
    "            ','] and i > first_noun]\n",
    "        if commas:\n",
    "            # TODO: this bails as soon as it finds one useful phrase; it should continue to\n",
    "            # backtrack and see if there's anything else to snip out!\n",
    "            phrase = ingred[commas[-1]+1:]\n",
    "            if phrase[0][1] in pos_distractions or phrase[-1][1] in pos_distractions or phrase[0][0] in distractions:\n",
    "                ingred = ingred[:commas[-1]]\n",
    "                return cls.focus([ingred, ingredient[1]])\n",
    "\n",
    "        # fix cruft in the lede phrase\n",
    "        noun_seen = False\n",
    "        distraction_start = None\n",
    "        lede_pos_distractions = ['VBD', 'VBN', 'DT']\n",
    "        lede_destractions = ['in']\n",
    "\n",
    "        for i, el in enumerate(ingred):\n",
    "            if not noun_seen and type(el[1]) is str and el[1].startswith('NN'):\n",
    "                noun_seen = True\n",
    "            elif noun_seen and (distraction_start is None) and (el[1] in lede_pos_distractions or el[0] in lede_destractions):\n",
    "                distraction_start = i\n",
    "            elif noun_seen and el[0] == ',':\n",
    "                break\n",
    "\n",
    "        if distraction_start is not None:\n",
    "            ingred = ingred[:distraction_start] + ingred[i+1:]\n",
    "\n",
    "        # there can also be cruft prior to the lede phrase; remove it\n",
    "        prior_phrases = [i for i in range(len(ingred)) if ingred[i][0] in [\n",
    "            ','] and i < first_noun]\n",
    "        non_amts = [i for i in range(len(ingred)) if ingred[i][1] not in [\n",
    "            cls.AMOUNT, cls.EA_AMOUNT, cls.ALT_AMOUNT]]\n",
    "        non_amt_start = min(non_amts) if non_amts else None\n",
    "        if prior_phrases and (non_amt_start is not None):\n",
    "            phrase = ingred[non_amt_start:prior_phrases[0]+1]\n",
    "            if len(phrase) == 1 or phrase[0][1] in distractions or phrase[-1][1] in distractions:\n",
    "                # snip it out\n",
    "                ingred = ingred[:non_amt_start] + ingred[prior_phrases[0]+1:]\n",
    "                return cls.focus([ingred, ingredient[1]])\n",
    "\n",
    "        return [ingred, ingredient[1]]\n",
    "\n",
    "    @classmethod\n",
    "    def unstop(cls, ingredient: list) -> list:\n",
    "        \"\"\" Remove stopwords and prep mods from parsed ingredient\"\"\"\n",
    "        out = []\n",
    "        prev_token = None\n",
    "        for token in ingredient[0]:\n",
    "            if type(token[0]) is str:\n",
    "                if any([token[0].lower() in stop for stop in [cls.STOPWORDS, cls.PREP_MODS]]):\n",
    "                    continue\n",
    "            if token[0] == ',' and prev_token in [',', None]:\n",
    "                prev_token = token[0]\n",
    "                continue\n",
    "            out += [token]\n",
    "            if type(token[0]) is str:\n",
    "                prev_token = token[0]\n",
    "        # clean up dangling cruft left by removal of stopwords\n",
    "        while out and out[-1][1] in ['CC', ',']:\n",
    "            del out[-1]\n",
    "        return [out, ingredient[1]]\n",
    "\n",
    "    @classmethod\n",
    "    def resolve_amounts(cls, ingredient: list) -> list:\n",
    "        resolved = []\n",
    "        for token in ingredient[0]:\n",
    "            if token[1] not in [cls.AMOUNT, cls.EA_AMOUNT, cls.ALT_AMOUNT]:\n",
    "                resolved += [token]\n",
    "                continue\n",
    "            qty, unit = cls.quantify(token[0])\n",
    "            resolved += [[[(qty, cls.QUANTITY), (unit, cls.UNIT)], token[1]]]\n",
    "        return [resolved, ingredient[1]]\n",
    "\n",
    "    # --------- Resolution of Parser.AMOUNTs and remnant strings\n",
    "    #  to definitive quantities and single strings --------- #\n",
    "\n",
    "    @classmethod\n",
    "    def standardize_unit(cls, unit):\n",
    "        \"\"\" Convert unit to its standard representation \"\"\"\n",
    "        if unit in cls.UNITS:\n",
    "            return cls.UNITS[unit]\n",
    "        elif unit[-1] == 's' and unit[:-1] in cls.UNITS:\n",
    "            return cls.UNITS[unit[:-1]]\n",
    "        elif unit[:-2] == 'es' and unit[:-2] in cls.UNITS:\n",
    "            return cls.UNITS[unit[:-2]]\n",
    "\n",
    "    @classmethod\n",
    "    def quantify(cls, amount):\n",
    "        \"\"\" Convert a Parser.AMOUNT object into a mass, volume, or `each` quantity,\n",
    "        return a (quantity, unit) tuple. mass's unit is `g`, volume is `ml`,\n",
    "        and each is `each`.\n",
    "\n",
    "        Mass is preferred to volume; volume is preferred to `each`. Parse the amount,\n",
    "        and standardize it to the most preferred option available.\n",
    "        \"\"\"\n",
    "        from convert import Convert\n",
    "        qty, unit = None, None\n",
    "        for el in amount[:2]:\n",
    "            if len(el) > 1:\n",
    "                if el[1] in [cls.QUANTITY, cls.EA_QUANTITY, cls.ALT_QUANTITY]:\n",
    "                    qty = float(el[0])\n",
    "                elif el[1] in [cls.UNIT, cls.EA_UNIT, cls.ALT_UNIT]:\n",
    "                    unit = el[0]\n",
    "\n",
    "        unit = cls.standardize_unit(unit)\n",
    "        if unit in Convert.VOLUME:\n",
    "            qty *= Convert.VOLUME[unit]\n",
    "            unit = 'ml'\n",
    "\n",
    "        elif unit in Convert.MASS:\n",
    "            qty *= Convert.MASS[unit]\n",
    "            unit = 'g'\n",
    "\n",
    "        alt_amount = None\n",
    "        plus_amount = None\n",
    "        ea_amount = None\n",
    "        for extra in amount[2:]:\n",
    "            if extra[1] == cls.ALT_AMOUNT:\n",
    "                alt_amount = cls.quantify(extra[0])\n",
    "            elif extra[1] == cls.EA_AMOUNT:\n",
    "                ea_amount = cls.quantify(extra[0])\n",
    "            elif extra[1] == cls.PLUS_AMOUNT:\n",
    "                plus_amount = cls.quantify(extra[0])\n",
    "\n",
    "        # the order here matters, as we want to convert in reverse\n",
    "        # order of priority, so that, for example, `ea` values get swapped out\n",
    "        # before we assess `g`-specific modifications\n",
    "        if unit in ['ea', 'pkg']:\n",
    "            if ea_amount:\n",
    "                qty *= ea_amount[0]\n",
    "                unit = ea_amount[1]\n",
    "            elif alt_amount and alt_amount[1] in ['g', 'ml']:\n",
    "                qty, unit = alt_amount\n",
    "\n",
    "        if unit == 'ml':\n",
    "            if alt_amount and alt_amount[1] == 'g':\n",
    "                qty, unit = alt_amount\n",
    "            elif plus_amount and plus_amount[1] == 'ml':\n",
    "                qty += plus_amount[0]\n",
    "\n",
    "        if unit == 'g':\n",
    "            # if we have a mass value, we only care about plus_amounts\n",
    "            if plus_amount and plus_amount[1] == 'g':\n",
    "                qty += plus_amount[0]\n",
    "\n",
    "        return qty, unit\n",
    "\n",
    "    @classmethod\n",
    "    def detokenize(cls, ingredient_tokens):\n",
    "        out = ''\n",
    "        for t in ingredient_tokens:\n",
    "            if out and t[1] not in [',']:\n",
    "                out += ' '\n",
    "            if t[1] in [cls.AMOUNT]:\n",
    "                out += f'{t[0][0][0]} {t[0][1][0]}'\n",
    "                # continue # TODO: reduce it to it's canonical representation\n",
    "            else:\n",
    "                try:\n",
    "                    out += t[0]\n",
    "                except Exception as err:\n",
    "                    print(t)\n",
    "                    raise err\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, raw_text: str):\n",
    "        \"\"\" Extract quantity and name information from an ingredient entry \"\"\"\n",
    "        text = cls.preprocess(raw_text)\n",
    "        if not text:\n",
    "            return None\n",
    "\n",
    "        tokens = cls.tokenize(text)\n",
    "        tokens = cls.decimate(tokens)\n",
    "        tokens = cls.rangeify(tokens)\n",
    "\n",
    "        tagged_data = cls.tag(tokens)\n",
    "        # TODO: figure out what to do about \"for example\", \"like\", \"such as\" phrases\n",
    "\n",
    "        # return tagged_data in the format it already knows!\n",
    "        if tagged_data:\n",
    "            tagged_data = tagged_data[0]  # only use the first entry for now\n",
    "            data = {\n",
    "                cls.QTYS: [],\n",
    "                cls.NAMES: '',\n",
    "                cls.MODS: [],\n",
    "                cls.STRIPPED_WORDS: []\n",
    "            }\n",
    "            while tagged_data[0] and tagged_data[0][0][1] == cls.AMOUNT:\n",
    "                amt = tagged_data[0].pop(0)\n",
    "                data[cls.QTYS] += [{el[1].lower(): el[0]\n",
    "                                   for el in amt[0]}]\n",
    "            if tagged_data: # remnant\n",
    "                data[cls.NAMES] = cls.detokenize(tagged_data[0][1:])\n",
    "        else:\n",
    "            data = None\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INPUT = [\n",
    "'grape or cherry tomatoes',\n",
    "'grape or cherry tomato',\n",
    "'ripe or green cherry tomato',\n",
    "'ripe, green cherry tomato',\n",
    "'6dingus 2 cup / 3',\n",
    "'6 2 cup / 3',\n",
    "'4 boneless, skinless chicken breasts (6 to 8 ounces each)',\n",
    "'unsalted, softened butter, softened: 9 tablespoons (4.5 ounces or 128 grams)',\n",
    "'4 large (8 to 9 ounces each) chicken breast halves, each cut in half crosswise to make 8 pieces total',\n",
    "'Two 5-ounce (140g) cans tuna in olive oil, drained (or 10 ounces/280g shredded roast chicken meat)',\n",
    "'1Â¼ cup/80 grams plus 2 teaspoons/5 grams mild honey',\n",
    "'1/2 pound fresh tuna, grilled or 6 1/2- to 7-ounce can albacore tuna, packed in water',\n",
    "'1 salmon or other firm fish, about 2 pounds, gutted and scaled, with the head left on',\n",
    "'4 whole fish, like sea bass or black bass, 1 to 1 1/2 pounds each', # PROBLEMATIC b/c we lose the whole \"like\" phrase\n",
    "'4 whole fish,, like sea bass or black bass, 1 to 1.5 pounds each', # PROBLEMATIC b/c we lose the whole \"like\" phrase\n",
    "'2 cans (10 ounces each) of condensed cream of celery or cream of mushroom soup',\n",
    "'2 cans (10 ounces/280 grams each) of condensed cream of celery or cream of mushroom soup',\n",
    "'1 six-to-eight-pound, cleaned, whole salmon, preferably with head left on (see note)',\n",
    "'1 (5-ounce) can tuna packed in olive oil, preferably Italian (see note)',\n",
    "'1 pound boneless, skinless chicken breast, cut across the grain in 1/4-inch thick slices',\n",
    "'2 4-pound Atlantic salmon (2 1/4 inches at thickest point), scaled and cleaned, gills removed, head and tail on, interior cavity well washed',\n",
    "'2 cotechini (Italian garlic sausages), about 1 pound each (available at Italian butcher shops)',\n",
    "'1Â¼ cup/80 grams mild honey',\n",
    "'1 (5- or 6-ounce) can or jar tuna, drained and flaked, or 1 (13-ounce) can chickpeas or white beans, drained',\n",
    "'2 (6-ounce) cans Italian tuna in water or oil, drained',\n",
    "'1 (4-ounce) can smoked mussels',\n",
    "'1Â¼ cup (approx. 80 grams) mild honey',\n",
    "'350g (approx. 1 1/2 cups) mild honey',\n",
    "'2 medium-size tomatoes, each seeded and cut into 6 pieces',\n",
    "'handful peanuts',\n",
    "'chopped parsley',\n",
    "'1-1/2 tablespoons Dijon mustard',\n",
    "'5-to-6-pound, cleaned, whole salmon, preferably with head left on (see note)',\n",
    "'1 5-to-6-pound, cleaned, whole salmon, preferably with head left on (see note)',\n",
    "'1 five- to six-pound, cleaned, whole salmon, preferably with head left on (see note)',\n",
    "'1 (1 1/2-pound) salmon fillet, skin-on or skinless',\n",
    "'6 cup',\n",
    "'6 cups',\n",
    "'6 cups <a href=\"https://cooking.nytimes.com/recipes/1021916-vegan-bolognese\">vegan Bolognese</a>',\n",
    "'1 cup flour',\n",
    "'1 c flour',\n",
    "'1.5c flour',\n",
    "'1 1/2c flour',\n",
    "'For the filling:  ',\n",
    "'1 packed cup cilantro, coarsely chopped',\n",
    "'4 (6-ounce) mild white fish fillets (for example, cod, hake or blackfish)',\n",
    "'1 (10- to 14-pound) turkey',\n",
    "'1 (10- to 14- pound) turkey',\n",
    "'1 salmon about 4 1/2 pounds, boned with head and tail left on',\n",
    "'1 scallion, chopped, for serving',\n",
    "'1/4 cup/80 grams mild honey',\n",
    "'1 or 2 cup',\n",
    "'5 to 7 handful',\n",
    "'yada yada',\n",
    "'chopped yada yada',\n",
    "'can or jar of stuff',\n",
    "'6c',\n",
    "'[6tsp]',\n",
    "'1 heaping cup',\n",
    "'1 trimmed stalk',\n",
    "'1 can or jar of stuff',\n",
    "'1 can/jar of stuff',\n",
    "'1 can/jar/tin of stuff',\n",
    "'1 can, jar, or tin of stuff',\n",
    "'1 can, jar or tin of stuff',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1.5-', 'JJ'), ('ounce', 'NN'), ('package', 'NN'), ('dried', 'VBD'), ('morels', 'NNS'), (',', ','), ('or', 'CC'), ('6.0', 'CD'), ('ounces', 'NNS'), ('fresh', 'JJ'), ('morels', 'NNS'), ('plus', 'CC'), ('0.5', 'CD'), ('cup', 'NN'), ('beef', 'NN'), ('broth', 'NN')]\n",
      "[('1.5-', 'JJ'), ('ounce', 'UNIT'), ('package', 'UNIT'), ('dried', 'VBD'), ('morels', 'NNS'), (',', ','), ('or', 'CC'), ('6.0', 'QTY'), ('ounces', 'UNIT'), ('fresh', 'JJ'), ('morels', 'NNS'), ('plus', 'CC'), ('0.5', 'QTY'), ('cup', 'UNIT'), ('beef', 'NN'), ('broth', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qtys': [{'qty': 42.524249999999995, 'unit': 'g'}],\n",
       " 'names': ['dried morels'],\n",
       " 'mods': []}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from parser import Parser\n",
    "\n",
    "# Parser.tag_amounts([('2-inch', 'JJ')])\n",
    "\n",
    "Parser.parse('1 1/2-ounce package dried morels, or 6 ounces fresh morels plus 1/2 cup beef broth')\n",
    "\n",
    "\n",
    "# for i, input in enumerate(TEST_INPUT):\n",
    "#     print('IN:', input)\n",
    "#     parsed = Parser.parse(input)\n",
    "#     print('PARSED:', parsed, '\\n')\n",
    "#     # print('OUT:', detokenize(parsed),'\\n')\n",
    "#     # print('{} -> {}\\n'.format(input, detokenize(parse(input))))\n",
    "#     # print('<----------------------------->\\n')\n",
    "#     if i == 5:\n",
    "#         # break\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 cup / 80 grams mild honey\n",
      "1/4 cup / 80 grams honey\n",
      "[{'unit': 'cup', 'qty': 0.25, 'qualifiers': [], 'per': None, 'plus': False}, {'unit': 'g', 'qty': 80.0, 'qualifiers': [], 'per': None, 'plus': False}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qtys': [{'unit': 'cup',\n",
       "   'qty': 0.25,\n",
       "   'qualifiers': [],\n",
       "   'per': None,\n",
       "   'plus': False},\n",
       "  {'unit': 'g', 'qty': 80.0, 'qualifiers': [], 'per': None, 'plus': False}],\n",
       " 'names': ['honey'],\n",
       " 'mods': [],\n",
       " 'stripped_words': ['mild']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parser import amounts\n",
    "\n",
    "amounts('1/4 cup/80 grams mild honey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grape', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('cherry', 'NN'),\n",
       " ('tomatoes', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "ing = 'grape or cherry tomatoes or some.'\n",
    "nltk.pos_tag(nltk.word_tokenize(ing))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
